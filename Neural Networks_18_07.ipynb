{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Perceptron\n",
    "Receives inputs, multiplies them by some weight, and then passes them into an activation function to produce an output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer #Binary classification dataset\n",
    "#Complete dataset- https://goo.gl/U2Uwz2\n",
    "cancer=load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer.keys() #its like a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "       'smoothness error', 'compactness error', 'concavity error',\n",
       "       'concave points error', 'symmetry error',\n",
       "       'fractal dimension error', 'worst radius', 'worst texture',\n",
       "       'worst perimeter', 'worst area', 'worst smoothness',\n",
       "       'worst compactness', 'worst concavity', 'worst concave points',\n",
       "       'worst symmetry', 'worst fractal dimension'], dtype='<U23')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Breast Cancer Wisconsin (Diagnostic) Database\\n=============================================\\n\\nNotes\\n-----\\nData Set Characteristics:\\n    :Number of Instances: 569\\n\\n    :Number of Attributes: 30 numeric, predictive attributes and the class\\n\\n    :Attribute Information:\\n        - radius (mean of distances from center to points on the perimeter)\\n        - texture (standard deviation of gray-scale values)\\n        - perimeter\\n        - area\\n        - smoothness (local variation in radius lengths)\\n        - compactness (perimeter^2 / area - 1.0)\\n        - concavity (severity of concave portions of the contour)\\n        - concave points (number of concave portions of the contour)\\n        - symmetry \\n        - fractal dimension (\"coastline approximation\" - 1)\\n\\n        The mean, standard error, and \"worst\" or largest (mean of the three\\n        largest values) of these features were computed for each image,\\n        resulting in 30 features.  For instance, field 3 is Mean Radius, field\\n        13 is Radius SE, field 23 is Worst Radius.\\n\\n        - class:\\n                - WDBC-Malignant\\n                - WDBC-Benign\\n\\n    :Summary Statistics:\\n\\n    ===================================== ====== ======\\n                                           Min    Max\\n    ===================================== ====== ======\\n    radius (mean):                        6.981  28.11\\n    texture (mean):                       9.71   39.28\\n    perimeter (mean):                     43.79  188.5\\n    area (mean):                          143.5  2501.0\\n    smoothness (mean):                    0.053  0.163\\n    compactness (mean):                   0.019  0.345\\n    concavity (mean):                     0.0    0.427\\n    concave points (mean):                0.0    0.201\\n    symmetry (mean):                      0.106  0.304\\n    fractal dimension (mean):             0.05   0.097\\n    radius (standard error):              0.112  2.873\\n    texture (standard error):             0.36   4.885\\n    perimeter (standard error):           0.757  21.98\\n    area (standard error):                6.802  542.2\\n    smoothness (standard error):          0.002  0.031\\n    compactness (standard error):         0.002  0.135\\n    concavity (standard error):           0.0    0.396\\n    concave points (standard error):      0.0    0.053\\n    symmetry (standard error):            0.008  0.079\\n    fractal dimension (standard error):   0.001  0.03\\n    radius (worst):                       7.93   36.04\\n    texture (worst):                      12.02  49.54\\n    perimeter (worst):                    50.41  251.2\\n    area (worst):                         185.2  4254.0\\n    smoothness (worst):                   0.071  0.223\\n    compactness (worst):                  0.027  1.058\\n    concavity (worst):                    0.0    1.252\\n    concave points (worst):               0.0    0.291\\n    symmetry (worst):                     0.156  0.664\\n    fractal dimension (worst):            0.055  0.208\\n    ===================================== ====== ======\\n\\n    :Missing Attribute Values: None\\n\\n    :Class Distribution: 212 - Malignant, 357 - Benign\\n\\n    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\\n\\n    :Donor: Nick Street\\n\\n    :Date: November, 1995\\n\\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\\nhttps://goo.gl/U2Uwz2\\n\\nFeatures are computed from a digitized image of a fine needle\\naspirate (FNA) of a breast mass.  They describe\\ncharacteristics of the cell nuclei present in the image.\\n\\nSeparating plane described above was obtained using\\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\\nConstruction Via Linear Programming.\" Proceedings of the 4th\\nMidwest Artificial Intelligence and Cognitive Science Society,\\npp. 97-101, 1992], a classification method which uses linear\\nprogramming to construct a decision tree.  Relevant features\\nwere selected using an exhaustive search in the space of 1-4\\nfeatures and 1-3 separating planes.\\n\\nThe actual linear program used to obtain the separating plane\\nin the 3-dimensional space is that described in:\\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\\nProgramming Discrimination of Two Linearly Inseparable Sets\",\\nOptimization Methods and Software 1, 1992, 23-34].\\n\\nThis database is also available through the UW CS ftp server:\\n\\nftp ftp.cs.wisc.edu\\ncd math-prog/cpo-dataset/machine-learn/WDBC/\\n\\nReferences\\n----------\\n   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \\n     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \\n     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\\n     San Jose, CA, 1993.\\n   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \\n     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \\n     July-August 1995.\\n   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\\n     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \\n     163-171.\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer['DESCR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cancer['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof(cancer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer['data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setting up our data- x and y variables\n",
    "x=cancer['data']\n",
    "y=cancer['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test= train_test_split(x,y,test_size=0.33, random_state=42) #proportion of test data and random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data pre-processing\n",
    "# Multi-layer Perceptron is sensitive to feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "scaler.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now apply the transformations to the data:\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(30, 30, 30), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=21, shuffle=True,\n",
       "       solver='sgd', tol=1e-09, validation_fraction=0.1, verbose=10,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for simplicity we will choose 3 layers\n",
    "#MLPClassifier trains using Backpropagation (Gradient Descent)\n",
    "mlp=MLPClassifier(hidden_layer_sizes= (30,30,30), max_iter=500, alpha=0.0001,\n",
    "                     solver='sgd', verbose=10,  random_state=21,tol=0.000000001) #three hidden layers with 30 units each\n",
    "mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(100, ), activation=’relu’, solver=’adam’, alpha=0.0001, batch_size=’auto’, learning_rate=’constant’, learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.72075737\n",
      "Iteration 2, loss = 0.71877200\n",
      "Iteration 3, loss = 0.71576550\n",
      "Iteration 4, loss = 0.71196943\n",
      "Iteration 5, loss = 0.70759033\n",
      "Iteration 6, loss = 0.70288249\n",
      "Iteration 7, loss = 0.69782504\n",
      "Iteration 8, loss = 0.69262888\n",
      "Iteration 9, loss = 0.68734754\n",
      "Iteration 10, loss = 0.68212515\n",
      "Iteration 11, loss = 0.67679281\n",
      "Iteration 12, loss = 0.67147887\n",
      "Iteration 13, loss = 0.66650535\n",
      "Iteration 14, loss = 0.66133396\n",
      "Iteration 15, loss = 0.65646619\n",
      "Iteration 16, loss = 0.65165437\n",
      "Iteration 17, loss = 0.64676365\n",
      "Iteration 18, loss = 0.64213597\n",
      "Iteration 19, loss = 0.63759865\n",
      "Iteration 20, loss = 0.63318775\n",
      "Iteration 21, loss = 0.62883557\n",
      "Iteration 22, loss = 0.62467666\n",
      "Iteration 23, loss = 0.62050001\n",
      "Iteration 24, loss = 0.61642903\n",
      "Iteration 25, loss = 0.61241744\n",
      "Iteration 26, loss = 0.60844988\n",
      "Iteration 27, loss = 0.60461326\n",
      "Iteration 28, loss = 0.60081861\n",
      "Iteration 29, loss = 0.59703917\n",
      "Iteration 30, loss = 0.59325898\n",
      "Iteration 31, loss = 0.58964278\n",
      "Iteration 32, loss = 0.58591975\n",
      "Iteration 33, loss = 0.58229122\n",
      "Iteration 34, loss = 0.57867959\n",
      "Iteration 35, loss = 0.57503091\n",
      "Iteration 36, loss = 0.57135953\n",
      "Iteration 37, loss = 0.56768905\n",
      "Iteration 38, loss = 0.56402287\n",
      "Iteration 39, loss = 0.56034477\n",
      "Iteration 40, loss = 0.55658890\n",
      "Iteration 41, loss = 0.55296139\n",
      "Iteration 42, loss = 0.54922418\n",
      "Iteration 43, loss = 0.54546196\n",
      "Iteration 44, loss = 0.54166128\n",
      "Iteration 45, loss = 0.53792102\n",
      "Iteration 46, loss = 0.53409447\n",
      "Iteration 47, loss = 0.53026829\n",
      "Iteration 48, loss = 0.52636239\n",
      "Iteration 49, loss = 0.52245348\n",
      "Iteration 50, loss = 0.51849693\n",
      "Iteration 51, loss = 0.51457344\n",
      "Iteration 52, loss = 0.51056633\n",
      "Iteration 53, loss = 0.50650505\n",
      "Iteration 54, loss = 0.50233259\n",
      "Iteration 55, loss = 0.49824387\n",
      "Iteration 56, loss = 0.49411694\n",
      "Iteration 57, loss = 0.48983842\n",
      "Iteration 58, loss = 0.48567620\n",
      "Iteration 59, loss = 0.48132906\n",
      "Iteration 60, loss = 0.47706172\n",
      "Iteration 61, loss = 0.47269088\n",
      "Iteration 62, loss = 0.46828178\n",
      "Iteration 63, loss = 0.46387186\n",
      "Iteration 64, loss = 0.45935385\n",
      "Iteration 65, loss = 0.45489290\n",
      "Iteration 66, loss = 0.45051755\n",
      "Iteration 67, loss = 0.44591653\n",
      "Iteration 68, loss = 0.44144132\n",
      "Iteration 69, loss = 0.43690965\n",
      "Iteration 70, loss = 0.43240498\n",
      "Iteration 71, loss = 0.42782266\n",
      "Iteration 72, loss = 0.42334989\n",
      "Iteration 73, loss = 0.41873795\n",
      "Iteration 74, loss = 0.41422909\n",
      "Iteration 75, loss = 0.40970231\n",
      "Iteration 76, loss = 0.40518836\n",
      "Iteration 77, loss = 0.40069722\n",
      "Iteration 78, loss = 0.39622454\n",
      "Iteration 79, loss = 0.39172038\n",
      "Iteration 80, loss = 0.38734471\n",
      "Iteration 81, loss = 0.38289038\n",
      "Iteration 82, loss = 0.37853998\n",
      "Iteration 83, loss = 0.37416113\n",
      "Iteration 84, loss = 0.36991033\n",
      "Iteration 85, loss = 0.36551177\n",
      "Iteration 86, loss = 0.36128232\n",
      "Iteration 87, loss = 0.35701889\n",
      "Iteration 88, loss = 0.35280133\n",
      "Iteration 89, loss = 0.34864613\n",
      "Iteration 90, loss = 0.34449746\n",
      "Iteration 91, loss = 0.34042733\n",
      "Iteration 92, loss = 0.33637759\n",
      "Iteration 93, loss = 0.33237163\n",
      "Iteration 94, loss = 0.32852311\n",
      "Iteration 95, loss = 0.32454001\n",
      "Iteration 96, loss = 0.32065420\n",
      "Iteration 97, loss = 0.31690994\n",
      "Iteration 98, loss = 0.31322374\n",
      "Iteration 99, loss = 0.30952062\n",
      "Iteration 100, loss = 0.30588688\n",
      "Iteration 101, loss = 0.30233432\n",
      "Iteration 102, loss = 0.29881978\n",
      "Iteration 103, loss = 0.29537815\n",
      "Iteration 104, loss = 0.29190709\n",
      "Iteration 105, loss = 0.28856172\n",
      "Iteration 106, loss = 0.28523656\n",
      "Iteration 107, loss = 0.28202005\n",
      "Iteration 108, loss = 0.27886754\n",
      "Iteration 109, loss = 0.27569559\n",
      "Iteration 110, loss = 0.27270704\n",
      "Iteration 111, loss = 0.26962984\n",
      "Iteration 112, loss = 0.26666301\n",
      "Iteration 113, loss = 0.26379901\n",
      "Iteration 114, loss = 0.26091119\n",
      "Iteration 115, loss = 0.25812320\n",
      "Iteration 116, loss = 0.25533242\n",
      "Iteration 117, loss = 0.25262743\n",
      "Iteration 118, loss = 0.24997789\n",
      "Iteration 119, loss = 0.24736567\n",
      "Iteration 120, loss = 0.24478254\n",
      "Iteration 121, loss = 0.24220891\n",
      "Iteration 122, loss = 0.23980411\n",
      "Iteration 123, loss = 0.23735012\n",
      "Iteration 124, loss = 0.23493035\n",
      "Iteration 125, loss = 0.23266460\n",
      "Iteration 126, loss = 0.23032655\n",
      "Iteration 127, loss = 0.22813315\n",
      "Iteration 128, loss = 0.22584505\n",
      "Iteration 129, loss = 0.22373597\n",
      "Iteration 130, loss = 0.22158708\n",
      "Iteration 131, loss = 0.21951644\n",
      "Iteration 132, loss = 0.21744267\n",
      "Iteration 133, loss = 0.21542176\n",
      "Iteration 134, loss = 0.21349946\n",
      "Iteration 135, loss = 0.21155794\n",
      "Iteration 136, loss = 0.20960093\n",
      "Iteration 137, loss = 0.20772597\n",
      "Iteration 138, loss = 0.20590978\n",
      "Iteration 139, loss = 0.20412747\n",
      "Iteration 140, loss = 0.20234571\n",
      "Iteration 141, loss = 0.20062495\n",
      "Iteration 142, loss = 0.19893207\n",
      "Iteration 143, loss = 0.19728147\n",
      "Iteration 144, loss = 0.19563470\n",
      "Iteration 145, loss = 0.19402865\n",
      "Iteration 146, loss = 0.19245249\n",
      "Iteration 147, loss = 0.19088953\n",
      "Iteration 148, loss = 0.18940745\n",
      "Iteration 149, loss = 0.18794075\n",
      "Iteration 150, loss = 0.18642703\n",
      "Iteration 151, loss = 0.18503499\n",
      "Iteration 152, loss = 0.18363523\n",
      "Iteration 153, loss = 0.18221426\n",
      "Iteration 154, loss = 0.18089138\n",
      "Iteration 155, loss = 0.17959650\n",
      "Iteration 156, loss = 0.17827375\n",
      "Iteration 157, loss = 0.17697984\n",
      "Iteration 158, loss = 0.17573628\n",
      "Iteration 159, loss = 0.17451727\n",
      "Iteration 160, loss = 0.17330426\n",
      "Iteration 161, loss = 0.17212741\n",
      "Iteration 162, loss = 0.17091032\n",
      "Iteration 163, loss = 0.16979093\n",
      "Iteration 164, loss = 0.16866554\n",
      "Iteration 165, loss = 0.16758702\n",
      "Iteration 166, loss = 0.16647645\n",
      "Iteration 167, loss = 0.16542000\n",
      "Iteration 168, loss = 0.16438055\n",
      "Iteration 169, loss = 0.16333863\n",
      "Iteration 170, loss = 0.16231924\n",
      "Iteration 171, loss = 0.16134504\n",
      "Iteration 172, loss = 0.16034519\n",
      "Iteration 173, loss = 0.15940855\n",
      "Iteration 174, loss = 0.15841641\n",
      "Iteration 175, loss = 0.15749647\n",
      "Iteration 176, loss = 0.15657438\n",
      "Iteration 177, loss = 0.15566600\n",
      "Iteration 178, loss = 0.15477217\n",
      "Iteration 179, loss = 0.15391989\n",
      "Iteration 180, loss = 0.15305500\n",
      "Iteration 181, loss = 0.15219101\n",
      "Iteration 182, loss = 0.15136368\n",
      "Iteration 183, loss = 0.15055406\n",
      "Iteration 184, loss = 0.14973977\n",
      "Iteration 185, loss = 0.14892266\n",
      "Iteration 186, loss = 0.14814992\n",
      "Iteration 187, loss = 0.14738288\n",
      "Iteration 188, loss = 0.14661601\n",
      "Iteration 189, loss = 0.14588013\n",
      "Iteration 190, loss = 0.14513825\n",
      "Iteration 191, loss = 0.14440262\n",
      "Iteration 192, loss = 0.14370488\n",
      "Iteration 193, loss = 0.14298897\n",
      "Iteration 194, loss = 0.14231872\n",
      "Iteration 195, loss = 0.14161334\n",
      "Iteration 196, loss = 0.14094605\n",
      "Iteration 197, loss = 0.14030060\n",
      "Iteration 198, loss = 0.13964978\n",
      "Iteration 199, loss = 0.13900675\n",
      "Iteration 200, loss = 0.13835167\n",
      "Iteration 201, loss = 0.13773994\n",
      "Iteration 202, loss = 0.13712135\n",
      "Iteration 203, loss = 0.13651037\n",
      "Iteration 204, loss = 0.13594064\n",
      "Iteration 205, loss = 0.13531684\n",
      "Iteration 206, loss = 0.13472206\n",
      "Iteration 207, loss = 0.13416372\n",
      "Iteration 208, loss = 0.13357905\n",
      "Iteration 209, loss = 0.13301268\n",
      "Iteration 210, loss = 0.13245917\n",
      "Iteration 211, loss = 0.13190914\n",
      "Iteration 212, loss = 0.13136964\n",
      "Iteration 213, loss = 0.13083274\n",
      "Iteration 214, loss = 0.13030588\n",
      "Iteration 215, loss = 0.12976531\n",
      "Iteration 216, loss = 0.12926038\n",
      "Iteration 217, loss = 0.12874998\n",
      "Iteration 218, loss = 0.12822435\n",
      "Iteration 219, loss = 0.12773540\n",
      "Iteration 220, loss = 0.12725060\n",
      "Iteration 221, loss = 0.12672689\n",
      "Iteration 222, loss = 0.12624841\n",
      "Iteration 223, loss = 0.12580595\n",
      "Iteration 224, loss = 0.12531190\n",
      "Iteration 225, loss = 0.12483668\n",
      "Iteration 226, loss = 0.12437372\n",
      "Iteration 227, loss = 0.12389867\n",
      "Iteration 228, loss = 0.12345340\n",
      "Iteration 229, loss = 0.12301536\n",
      "Iteration 230, loss = 0.12255792\n",
      "Iteration 231, loss = 0.12212177\n",
      "Iteration 232, loss = 0.12169187\n",
      "Iteration 233, loss = 0.12123961\n",
      "Iteration 234, loss = 0.12084924\n",
      "Iteration 235, loss = 0.12040685\n",
      "Iteration 236, loss = 0.11997980\n",
      "Iteration 237, loss = 0.11958901\n",
      "Iteration 238, loss = 0.11917283\n",
      "Iteration 239, loss = 0.11876903\n",
      "Iteration 240, loss = 0.11837874\n",
      "Iteration 241, loss = 0.11798045\n",
      "Iteration 242, loss = 0.11756789\n",
      "Iteration 243, loss = 0.11717419\n",
      "Iteration 244, loss = 0.11681034\n",
      "Iteration 245, loss = 0.11642676\n",
      "Iteration 246, loss = 0.11602497\n",
      "Iteration 247, loss = 0.11565043\n",
      "Iteration 248, loss = 0.11527307\n",
      "Iteration 249, loss = 0.11492601\n",
      "Iteration 250, loss = 0.11452724\n",
      "Iteration 251, loss = 0.11418106\n",
      "Iteration 252, loss = 0.11380400\n",
      "Iteration 253, loss = 0.11344070\n",
      "Iteration 254, loss = 0.11307876\n",
      "Iteration 255, loss = 0.11272104\n",
      "Iteration 256, loss = 0.11238317\n",
      "Iteration 257, loss = 0.11202563\n",
      "Iteration 258, loss = 0.11167843\n",
      "Iteration 259, loss = 0.11132553\n",
      "Iteration 260, loss = 0.11100020\n",
      "Iteration 261, loss = 0.11065464\n",
      "Iteration 262, loss = 0.11031254\n",
      "Iteration 263, loss = 0.10998186\n",
      "Iteration 264, loss = 0.10962509\n",
      "Iteration 265, loss = 0.10930465\n",
      "Iteration 266, loss = 0.10899429\n",
      "Iteration 267, loss = 0.10864337\n",
      "Iteration 268, loss = 0.10831735\n",
      "Iteration 269, loss = 0.10801385\n",
      "Iteration 270, loss = 0.10769468\n",
      "Iteration 271, loss = 0.10738324\n",
      "Iteration 272, loss = 0.10706039\n",
      "Iteration 273, loss = 0.10674211\n",
      "Iteration 274, loss = 0.10644685\n",
      "Iteration 275, loss = 0.10614201\n",
      "Iteration 276, loss = 0.10583716\n",
      "Iteration 277, loss = 0.10553716\n",
      "Iteration 278, loss = 0.10522978\n",
      "Iteration 279, loss = 0.10492773\n",
      "Iteration 280, loss = 0.10464185\n",
      "Iteration 281, loss = 0.10435224\n",
      "Iteration 282, loss = 0.10406833\n",
      "Iteration 283, loss = 0.10377047\n",
      "Iteration 284, loss = 0.10349678\n",
      "Iteration 285, loss = 0.10320657\n",
      "Iteration 286, loss = 0.10292281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 287, loss = 0.10263962\n",
      "Iteration 288, loss = 0.10237687\n",
      "Iteration 289, loss = 0.10210508\n",
      "Iteration 290, loss = 0.10181180\n",
      "Iteration 291, loss = 0.10156042\n",
      "Iteration 292, loss = 0.10128266\n",
      "Iteration 293, loss = 0.10102878\n",
      "Iteration 294, loss = 0.10076204\n",
      "Iteration 295, loss = 0.10048057\n",
      "Iteration 296, loss = 0.10022400\n",
      "Iteration 297, loss = 0.09996555\n",
      "Iteration 298, loss = 0.09971431\n",
      "Iteration 299, loss = 0.09944688\n",
      "Iteration 300, loss = 0.09921263\n",
      "Iteration 301, loss = 0.09896147\n",
      "Iteration 302, loss = 0.09868431\n",
      "Iteration 303, loss = 0.09845762\n",
      "Iteration 304, loss = 0.09818905\n",
      "Iteration 305, loss = 0.09795503\n",
      "Iteration 306, loss = 0.09771231\n",
      "Iteration 307, loss = 0.09747171\n",
      "Iteration 308, loss = 0.09722014\n",
      "Iteration 309, loss = 0.09700061\n",
      "Iteration 310, loss = 0.09675869\n",
      "Iteration 311, loss = 0.09650620\n",
      "Iteration 312, loss = 0.09628463\n",
      "Iteration 313, loss = 0.09604858\n",
      "Iteration 314, loss = 0.09581305\n",
      "Iteration 315, loss = 0.09558239\n",
      "Iteration 316, loss = 0.09535229\n",
      "Iteration 317, loss = 0.09512754\n",
      "Iteration 318, loss = 0.09490527\n",
      "Iteration 319, loss = 0.09467020\n",
      "Iteration 320, loss = 0.09444243\n",
      "Iteration 321, loss = 0.09423516\n",
      "Iteration 322, loss = 0.09401556\n",
      "Iteration 323, loss = 0.09378296\n",
      "Iteration 324, loss = 0.09357680\n",
      "Iteration 325, loss = 0.09333520\n",
      "Iteration 326, loss = 0.09312791\n",
      "Iteration 327, loss = 0.09292116\n",
      "Iteration 328, loss = 0.09269284\n",
      "Iteration 329, loss = 0.09248775\n",
      "Iteration 330, loss = 0.09228288\n",
      "Iteration 331, loss = 0.09207295\n",
      "Iteration 332, loss = 0.09184954\n",
      "Iteration 333, loss = 0.09165166\n",
      "Iteration 334, loss = 0.09143654\n",
      "Iteration 335, loss = 0.09123821\n",
      "Iteration 336, loss = 0.09103894\n",
      "Iteration 337, loss = 0.09083311\n",
      "Iteration 338, loss = 0.09063820\n",
      "Iteration 339, loss = 0.09043243\n",
      "Iteration 340, loss = 0.09023908\n",
      "Iteration 341, loss = 0.09004445\n",
      "Iteration 342, loss = 0.08984450\n",
      "Iteration 343, loss = 0.08965053\n",
      "Iteration 344, loss = 0.08944671\n",
      "Iteration 345, loss = 0.08924981\n",
      "Iteration 346, loss = 0.08906978\n",
      "Iteration 347, loss = 0.08887361\n",
      "Iteration 348, loss = 0.08868112\n",
      "Iteration 349, loss = 0.08850618\n",
      "Iteration 350, loss = 0.08831376\n",
      "Iteration 351, loss = 0.08812647\n",
      "Iteration 352, loss = 0.08793422\n",
      "Iteration 353, loss = 0.08773841\n",
      "Iteration 354, loss = 0.08756422\n",
      "Iteration 355, loss = 0.08738209\n",
      "Iteration 356, loss = 0.08718704\n",
      "Iteration 357, loss = 0.08700758\n",
      "Iteration 358, loss = 0.08685411\n",
      "Iteration 359, loss = 0.08664186\n",
      "Iteration 360, loss = 0.08646694\n",
      "Iteration 361, loss = 0.08629834\n",
      "Iteration 362, loss = 0.08609731\n",
      "Iteration 363, loss = 0.08591378\n",
      "Iteration 364, loss = 0.08574850\n",
      "Iteration 365, loss = 0.08556296\n",
      "Iteration 366, loss = 0.08539646\n",
      "Iteration 367, loss = 0.08521682\n",
      "Iteration 368, loss = 0.08504220\n",
      "Iteration 369, loss = 0.08485224\n",
      "Iteration 370, loss = 0.08467962\n",
      "Iteration 371, loss = 0.08451313\n",
      "Iteration 372, loss = 0.08433047\n",
      "Iteration 373, loss = 0.08416231\n",
      "Iteration 374, loss = 0.08398065\n",
      "Iteration 375, loss = 0.08382176\n",
      "Iteration 376, loss = 0.08364856\n",
      "Iteration 377, loss = 0.08347795\n",
      "Iteration 378, loss = 0.08332496\n",
      "Iteration 379, loss = 0.08314828\n",
      "Iteration 380, loss = 0.08298802\n",
      "Iteration 381, loss = 0.08281950\n",
      "Iteration 382, loss = 0.08265958\n",
      "Iteration 383, loss = 0.08249643\n",
      "Iteration 384, loss = 0.08233040\n",
      "Iteration 385, loss = 0.08217961\n",
      "Iteration 386, loss = 0.08201984\n",
      "Iteration 387, loss = 0.08184914\n",
      "Iteration 388, loss = 0.08169312\n",
      "Iteration 389, loss = 0.08154664\n",
      "Iteration 390, loss = 0.08138716\n",
      "Iteration 391, loss = 0.08123206\n",
      "Iteration 392, loss = 0.08106900\n",
      "Iteration 393, loss = 0.08091040\n",
      "Iteration 394, loss = 0.08077070\n",
      "Iteration 395, loss = 0.08060780\n",
      "Iteration 396, loss = 0.08045416\n",
      "Iteration 397, loss = 0.08030334\n",
      "Iteration 398, loss = 0.08015283\n",
      "Iteration 399, loss = 0.08000965\n",
      "Iteration 400, loss = 0.07986093\n",
      "Iteration 401, loss = 0.07971961\n",
      "Iteration 402, loss = 0.07956250\n",
      "Iteration 403, loss = 0.07941379\n",
      "Iteration 404, loss = 0.07926401\n",
      "Iteration 405, loss = 0.07913665\n",
      "Iteration 406, loss = 0.07899512\n",
      "Iteration 407, loss = 0.07883560\n",
      "Iteration 408, loss = 0.07869498\n",
      "Iteration 409, loss = 0.07855969\n",
      "Iteration 410, loss = 0.07841188\n",
      "Iteration 411, loss = 0.07826308\n",
      "Iteration 412, loss = 0.07813909\n",
      "Iteration 413, loss = 0.07799248\n",
      "Iteration 414, loss = 0.07785976\n",
      "Iteration 415, loss = 0.07770709\n",
      "Iteration 416, loss = 0.07757743\n",
      "Iteration 417, loss = 0.07743547\n",
      "Iteration 418, loss = 0.07730562\n",
      "Iteration 419, loss = 0.07716678\n",
      "Iteration 420, loss = 0.07703842\n",
      "Iteration 421, loss = 0.07690578\n",
      "Iteration 422, loss = 0.07676408\n",
      "Iteration 423, loss = 0.07664264\n",
      "Iteration 424, loss = 0.07649653\n",
      "Iteration 425, loss = 0.07636325\n",
      "Iteration 426, loss = 0.07622675\n",
      "Iteration 427, loss = 0.07610085\n",
      "Iteration 428, loss = 0.07596405\n",
      "Iteration 429, loss = 0.07583015\n",
      "Iteration 430, loss = 0.07570493\n",
      "Iteration 431, loss = 0.07559266\n",
      "Iteration 432, loss = 0.07544811\n",
      "Iteration 433, loss = 0.07532001\n",
      "Iteration 434, loss = 0.07520130\n",
      "Iteration 435, loss = 0.07507289\n",
      "Iteration 436, loss = 0.07494811\n",
      "Iteration 437, loss = 0.07482980\n",
      "Iteration 438, loss = 0.07471442\n",
      "Iteration 439, loss = 0.07458007\n",
      "Iteration 440, loss = 0.07445922\n",
      "Iteration 441, loss = 0.07433997\n",
      "Iteration 442, loss = 0.07422226\n",
      "Iteration 443, loss = 0.07410616\n",
      "Iteration 444, loss = 0.07397744\n",
      "Iteration 445, loss = 0.07386977\n",
      "Iteration 446, loss = 0.07375862\n",
      "Iteration 447, loss = 0.07363268\n",
      "Iteration 448, loss = 0.07351982\n",
      "Iteration 449, loss = 0.07340681\n",
      "Iteration 450, loss = 0.07329069\n",
      "Iteration 451, loss = 0.07318180\n",
      "Iteration 452, loss = 0.07307066\n",
      "Iteration 453, loss = 0.07295112\n",
      "Iteration 454, loss = 0.07284476\n",
      "Iteration 455, loss = 0.07273249\n",
      "Iteration 456, loss = 0.07262136\n",
      "Iteration 457, loss = 0.07251059\n",
      "Iteration 458, loss = 0.07240667\n",
      "Iteration 459, loss = 0.07228716\n",
      "Iteration 460, loss = 0.07218162\n",
      "Iteration 461, loss = 0.07207844\n",
      "Iteration 462, loss = 0.07197077\n",
      "Iteration 463, loss = 0.07185916\n",
      "Iteration 464, loss = 0.07174754\n",
      "Iteration 465, loss = 0.07164649\n",
      "Iteration 466, loss = 0.07155632\n",
      "Iteration 467, loss = 0.07144328\n",
      "Iteration 468, loss = 0.07132792\n",
      "Iteration 469, loss = 0.07122948\n",
      "Iteration 470, loss = 0.07111882\n",
      "Iteration 471, loss = 0.07102992\n",
      "Iteration 472, loss = 0.07091704\n",
      "Iteration 473, loss = 0.07082118\n",
      "Iteration 474, loss = 0.07070317\n",
      "Iteration 475, loss = 0.07060364\n",
      "Iteration 476, loss = 0.07050314\n",
      "Iteration 477, loss = 0.07041146\n",
      "Iteration 478, loss = 0.07030241\n",
      "Iteration 479, loss = 0.07019529\n",
      "Iteration 480, loss = 0.07009288\n",
      "Iteration 481, loss = 0.06999722\n",
      "Iteration 482, loss = 0.06990138\n",
      "Iteration 483, loss = 0.06979736\n",
      "Iteration 484, loss = 0.06969923\n",
      "Iteration 485, loss = 0.06960159\n",
      "Iteration 486, loss = 0.06949685\n",
      "Iteration 487, loss = 0.06939351\n",
      "Iteration 488, loss = 0.06930396\n",
      "Iteration 489, loss = 0.06919860\n",
      "Iteration 490, loss = 0.06910860\n",
      "Iteration 491, loss = 0.06901545\n",
      "Iteration 492, loss = 0.06890476\n",
      "Iteration 493, loss = 0.06881293\n",
      "Iteration 494, loss = 0.06872486\n",
      "Iteration 495, loss = 0.06863580\n",
      "Iteration 496, loss = 0.06852483\n",
      "Iteration 497, loss = 0.06842653\n",
      "Iteration 498, loss = 0.06834119\n",
      "Iteration 499, loss = 0.06825295\n",
      "Iteration 500, loss = 0.06814581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gautam\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(30, 30, 30), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=21, shuffle=True,\n",
       "       solver='sgd', tol=1e-09, validation_fraction=0.1, verbose=10,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the training data to our model\n",
    "mlp.fit(x_train,y_train) #the model will accept a bunch of default values for other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " predictions=mlp.predict(x_test) #run predictions on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[43  1]\n",
      " [ 2 97]]\n"
     ]
    }
   ],
   "source": [
    "#use sklearn's built in metrics for confusion matrix\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.98      0.97        44\n",
      "          1       0.99      0.98      0.98        99\n",
      "\n",
      "avg / total       0.98      0.98      0.98       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
